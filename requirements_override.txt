# We are overriding the "torch" package version with a
# specific compatible version that also supports CUDA.
--extra-index-url https://download.pytorch.org/whl/cu124
torch==2.4.0+cu124

# We are installing the missing "tiktoken" package
# to enable support for the GLM model architecture.
tiktoken==0.10.0

# We are importing the llama.cpp project dependencies.
--requirement ./vendor/llama.cpp/requirements.txt
