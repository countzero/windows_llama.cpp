# We are overriding the "torch" package version with a
# specific compatible version that also supports CUDA.
--extra-index-url https://download.pytorch.org/whl/cu121
torch==2.2.1+cu121

# We are installing the missing "tiktoken" package
# to enable support for the GLM model architecture.
tiktoken==0.7.0

# We are importing the llama.cpp project dependencies.
--requirement ./vendor/llama.cpp/requirements.txt
