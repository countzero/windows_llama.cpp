# We are importing the llama.cpp project dependencies.
--requirement ./vendor/llama.cpp/requirements.txt

# We are overriding the "torch" package version with a
# specific compatible version that also supports CUDA.
--extra-index-url https://download.pytorch.org/whl/cu121
torch==2.1.2+cu121
